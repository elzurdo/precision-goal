We examine simulation results for three scenarios:
\begin{enumerate}
  \item \textbf{Fair Coin ($\theta_{\rm true}=0.5$)}: The null hypothesis is true.
  \item \textbf{Highly Loaded Coin ($\theta_{\rm true}=0.6$)}: A strong effect exists.
  \item \textbf{Slightly Loaded Coins ($\theta_{\rm true}=0.53, 0.57$)}: Weak effects exist near the ROPE boundary.  challenging the sensitivity of the algorithms
\end{enumerate}

\subsection{Fair Coin ($\theta_{\rm true}=0.5$)}\label{sec:fair_coin}

Figure \ref{fig:iterations} presents the results of a hand-picked fair coin experiment.\footnote{This figure is similar to the top panels in \cite{kruschke2015doing} Figures 13.4 and 13.5. \textcolor{red}{\textbf{[TODO: provide the exact sequence]}}}
It was chosen to highlight potential stark differences in outcomes between the three algorithms. Note that the posteriors shown in Figure \ref{fig:posteriors} correspond to three specific iterations of this single experiment.

As seen in Figure \ref{fig:iterations}, at iteration 126, the 95\% HDI of the \textit{HDI+ROPE} algorithm falls fully outside the ROPE, causing it to confidently---though incorrectly---reject $\theta_{\rm null}$.

In contrast, the \textit{Precision is the Goal} (PitG) stop criterion is met at iteration 598, when the HDI width becomes narrower than the goal of 0.08. However, because the HDI still straddles the ROPE boundary, the decision is inconclusive.

Finally, for \textit{Enhanced Precision is the Goal} (ePitG), the stop criterion is only met at iteration 804, when the HDI satisfies both conditions: it is narrower than the precision goal \textit{and} falls fully within the ROPE. This results in a correct acceptance of $\theta_{\rm null}$.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{cherry_iterations.png}
  \caption{Hand-picked fair coin sample demonstrating variate outcomes by stop criterion. The vertical axis is the cumulative success rate at each iteration. The gray band is the 95\% HDI. Highlighted iterations indicate stop triggers: Vertical red dashed lines = Reject $\theta_{\rm null}$; Vertical green solid lines = Accept $\theta_{\rm null}$. Purple dots indicate when the precision goal is achieved. Note: HDI+ROPE stops at iteration 126 (Recall top panel of Figure \ref{fig:posteriors}), PitG stops at iteration 598 (first purple dot; middle panel of Figure \ref{fig:posteriors}), and ePitG stops at iteration 804 (green line; bottom panel of Figure \ref{fig:posteriors}). ROPE boundaries are dashed horizontal lines.}
  \label{fig:iterations}
\end{figure}

We expand this analysis to $M=500$ experiments, displaying outcomes in Figure \ref{fig:fair_iter_vs_rate} and summarizing key statistics in Table \ref{tab:fair_iter_vs_rate}. \footnote{This visual is inspired by \cite{kruschke2015doing}, Figure 13.6.}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{fair_experiments_iter_vs_rate.png}
  \caption{Outcomes of 500 experiments. Small symbols represent individual experiment stops; large symbols represent means. Red squares: HDI+ROPE. Blue circles: PitG. Green Xs: ePitG. Solid line: $\theta_{\rm true}$. Dashed lines: ROPE boundaries. \textcolor{red}{\textbf{[TK: analytical results]}}}
  \label{fig:fair_iter_vs_rate}
\end{figure}

In Figure \ref{fig:fair_iter_vs_rate}, the large symbols representing the means of 500 experiments show that, on average, all methods yield unbiased outcomes (see the $\overline{\theta}$ column in Table \ref{tab:fair_iter_vs_rate}).

TODO: consider using median and percentiles instead of mean and standard deviation, as these are more robust to outliers. This is especially relevant for the HDI+ROPE method, which has a large variance in stop iteration due to early stopping on extreme samples.

\begin{table}[h!]\label{tab:fair_iter_vs_rate}
  \begin{center}
  \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
    Algorithm & Accept & Reject & Conclusive & Inconclusive & $\overline{n}$ & $\sigma_n$ & $\overline{\theta}$ & $\sigma_{\hat{\theta}}$\\
    \hline
    HDI+ROPE & 0.928 & 0.06 & 0.988 & 0.012 & 576.6 & 281 & 0.4952 & 0.0502 \\
    PitG & 0.390 & 0.00 & 0.390 & 0.610 & 598.3 & 1.4 & 0.4994 & 0.01997 \\
    ePitG & 0.984 & 0.00 & 0.984 & 0.016 & 732.2 & 210 & 0.4998 & 0.0127\\
    \hline
  \end{tabular}
  \caption{Statistical summaries of 500 fair coin experiments (Figure \ref{fig:fair_iter_vs_rate}).
  Mean stop iteration $\overline{n}$;
  Mean sample rate at stop $\overline{\theta}$.
  Note: \textit{Accept} + \textit{Reject} = \textit{Conclusive}; \textit{Conclusive} + \textit{Inconclusive} = 1.}
\end{center}
\end{table}

However, HDI+ROPE falsely rejects $\theta_{\rm null}$ at a rate of 6\%, whereas the precision-based methods exhibit zero False Positives (see Reject column in Table \ref{tab:fair_iter_vs_rate}).\footnote{If we relax the minimum sample size ($N_{\rm min}=0$) for HDI+ROPE, the False Positive Rate increases to \textcolor{red}{\textbf{[TK\%]}}.}

Precision is the Goal (blue circles) stops on average at iteration $\sim 598$, but 61\% of decisions remain inconclusive. For ePitG (green Xs), achieving conclusiveness requires additional sampling, resulting in only 1.6\% inconclusive outcomes by the final iteration ($N_{\rm max}=1,500$). This manifests as an average of $732.2\pm 210$ samples per experiment for ePitG compared to $598.3\pm 1.4$ for PitG---an average increase of 22.4\%.

Figure \ref{fig:fair_iter_vs_rate} highlights that after the ``Precision barrier'' at calcuated at 599.2, ePitG and HDI+ROPE become effectively the same algorithm, as the precision goal has been satisfied.

In Figure \ref{fig:fair_decisions}, we explore the decision rates over time.\footnote{Display inspired by top panels of Figure 13.6 in \cite{kruschke2015doing}.}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{fair_experiment_decision_rates.png}
  \caption{Cumulative distribution of decisions. Left: HDI+ROPE. Right: Precision-based methods. Vertical axis: Proportion of decisions. Sum of Reject (red dashed), Accept (green solid), and Inconclusive (gray dot-dashed) is 100\% at each iteration.}
  \label{fig:fair_decisions}
\end{figure}

The left panel of Figure \ref{fig:fair_decisions} shows that for low $n$, HDI+ROPE may reject $\theta_{\rm null}$ (6\% FPR), which precision methods avoid (right panel). Around iteration 400 \textcolor{red}{\textbf{[TK: to be calculated]}}, HDI+ROPE gradually shifts toward accepting $\theta_{\rm null}$. In Figure \ref{fig:fair_iter_vs_rate}, this appears as red squares centering on $\theta_{\rm true}=0.5$ and then spreading toward the ROPE boundaries. This occurs because while the posterior mode may deviate slightly from truth, the narrowing interval eventually falls entirely within the ROPE.

The right panel of Figure \ref{fig:fair_decisions} confirms that Precision is the Goal triggers at iteration 598 with extremely low variance ($\sigma_n=1.4$). Consequently, only 39\% of results are decisive (all correctly accepting $\theta_{\rm null}$).

Enhanced PitG (green Xs) extends the unbiased promise of PitG but is significantly more decisive. The ``accept'' curve in the right panel grows steadily to 98.4\% by $N_{\rm max}=1,500$.

As this binomial setup is well-defined, we observe trends that can be described analytically. We discuss this in Section \textcolor{red}{\textbf{[TK]}}, but note here that the expected stop iteration for Precision is the Goal is approximated by:
$$n \approx \frac{4 z_{*}^2}{\rm{CI}^2}p(1-p) - 1$$
where:
\begin{itemize}
  \item $p$ is the success rate. \textcolor{red}{\textbf{[TK: Perhaps this should be $\theta_{\rm true}$?]}}
  \item ${\rm CI}$ is the precision goal (here 0.08).
  \item $z_{*}$ is the critical value (e.g., $z_{0.05}=1.96$). \textcolor{red}{\textbf{[TK: confirm variable. For some reason I wote "E.g, for 95\% quantile of the standard normal distribution"]}}
\end{itemize}

For this fair coin instance ($p=0.5$), we obtain $n \approx 599.2$, indicated by the dashed vertical line in Figure \ref{fig:fair_iter_vs_rate}.

We now turn to examining a loaded coin example where this is not the case.

\subsection{Loaded Coin ($\theta_{\rm true}=0.6$)}\label{sec:loaded_coin}

This experiment represents a case where the null hypothesis is false and there is a fairly strong effect. Results for these experiments are displayed in Figures \ref{fig:loaded0pt6_iter_vs_rate} and \ref{fig:loaded0pt6_decisions}, as well as Table \ref{tab:loaded0pt6_iter_vs_rate}, which are similar to those presented for the fair coin case in Section \ref{sec:fair_coin}.

We observe that for all three algorithms, there is a 0\% False Negative Rate (see Accept column in Table \ref{tab:loaded0pt6_iter_vs_rate}); none of the experiments incorrectly accepted the null hypothesis.

The \textit{HDI+ROPE} algorithm, however, is highly biased, with a mean success rate at stopping of $\overline{\theta} \approx 0.644$. This bias arises because many experiments stop very early, resulting in a mean stop iteration of $\overline{n} \approx 268$ (with a large standard deviation of $\sigma_n=291$), which is less than half the expected precision goal value of $N_{\theta_{\rm true}}=575.2$.

Both precision-based algorithms yield unbiased outcomes. However, \textit{Precision is the Goal} (PitG) remains somewhat indecisive, with 29.2\% of experiments ending as inconclusive. In contrast, \textit{Enhanced Precision is the Goal} (ePitG) is significantly more decisive, clarifying the outcome in 99.8\% of cases (compared to 98.4\% for the fair coin). In this scenario, ePitG requires only 10.7\% more samples than PitG on average.

The symmetry observed in the fair coin case (Figure \ref{fig:fair_iter_vs_rate}) around $\theta_{\rm true}=0.5$ is, as expected, absent here. In Figure \ref{fig:loaded0pt6_iter_vs_rate}, this manifests in the sharp slope of the PitG results (blue circles), which asymptote towards the precision goal value of $N_{\theta_{\rm true}}=575.2$ (dashed vertical line) from above ($\hat{\theta} \ge \theta_{\rm true}$) and not from the bottom.
That said, symmetry of this problem dicates that results for a loaded coine with value $\theta_{\rm true}=0.4$ would be the mirror image of those for $\theta_{\rm true}=0.6$.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{loaded0pt6_experiments_iter_vs_rate.png}
  \caption{Outcomes of 500 experiments with loaded coin $\theta_{\rm true}=0.6$. Layout and symbols follow Figure \ref{fig:fair_iter_vs_rate}.}
  \label{fig:loaded0pt6_iter_vs_rate}
\end{figure}

\begin{table}[h!]\label{tab:loaded0pt6_iter_vs_rate}
  \begin{center}
  \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
    Algorithm & Accept & Reject & Conclusive & Inconclusive & $\overline{n}$ & $\sigma_n$ & $\overline{\theta}$ & $\sigma_{\hat{\theta}}$\\
    \hline
    HDI+ROPE & 0.0	& 0.998	& 0.998 &	0.002	& 268.3 &	290.8 & 0.6441 &	0.0539 \\
    PitG & 0.0 &	0.708 &	0.708 &	0.292	& 573.7	& 10.2 &	0.6011 &	0.02038 \\
    ePitG & 0.0	& 0.998	& 0.998	& 0.002	& 634.9	& 149	& 0.6034	 & 0.0174 \\
    \hline
  \end{tabular}
  \caption{Statistical summaries of 500 loaded coin experiments ($\theta_{\rm true}=0.6$). Compare with the fair coin baseline in Table \ref{tab:fair_iter_vs_rate}.
  Note: \textit{Accept} + \textit{Reject} = \textit{Conclusive}; \textit{Conclusive} + \textit{Inconclusive} = 1.}
\end{center}
\end{table}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{loaded0pt6_experiment_decision_rates.png}
  \caption{Cumulative distribution of decisions for $\theta_{\rm true}=0.6$. Compare with Figure \ref{fig:fair_decisions}. Note the high decisiveness of ePitG (green, right panel) compared to PitG (blue, right panel) and the early stopping bias of HDI+ROPE (red, left panel).}
  \label{fig:loaded0pt6_decisions}
\end{figure}

\subsection{Close to the ROPE ($\theta_{\rm true}=0.53, 0.57$)}

We now explore the cases of $\theta_{\rm true}=0.57$ and $0.53$
where we expect a lot of inconclusive outcomes due to straddlers of the ROPE.


\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{loaded0pt57_experiments_iter_vs_rate.png}
  \caption{Similar to Figure \ref{fig:fair_iter_vs_rate} but with
  loaded coin $\theta_{\rm true}=0.57$.
  Small symbols are individual experiment outcomes. Large are experiments
  mean values. When using HDI+ROPE as the stop criterion results in the red squares.
  Precision is the Goal as the criterion results in the blue circles
  and the Enhanced PitG as the green Xs. $\theta_{\rm true}$ is the solid horizontal line and
  the ROPE is the horizontaldashed lines. Summary stats are in Table \ref{tab:fair_iter_vs_rate}. TK: analytical results
  }
  \label{fig:loaded0pt57_iter_vs_rate}
\end{figure}


\begin{table}[h!]\label{tab:loaded0pt57_iter_vs_rate}
  \begin{center}
  \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
    Algorithm & Accept & Reject & Conclusive & Inconclusive & $\overline{n}$ & $\sigma_n$ & $\overline{\theta}$ & $\sigma_{\hat{\theta}}$\\
    \hline
    HDI+ROPE & 0.0080	& 0.648 &	0.656 &	0.344 &	804.6	& 611.0	& 0.605	& 0.058 \\
    PitG & 0.0005 &	0.157 &	0.158 &	0.843	& 586.5	& 7.1 &	0.570 &	0.020\\
    ePitG & 0.0035 &	0.542 &	0.545 & 	0.455 &	1127.2 &	395.9	& 0.576 &	0.017  \\
    \hline
  \end{tabular}
  \caption{Similar to Table \ref{tab:fair_iter_vs_rate} but for $\theta_{\rm true}=0.57$ Statistic summaries of 500 experiments of the three stop criteria shown in
  Figure \ref{fig:fair_iter_vs_rate}. {\it Accept}
  is the fraction of experiments which results in acceptence of $\theta_{\rm null}$,
  and similar in reverse for {\it Reject}. {\it Conclusive} is the sum of Accept
  and Reject and {\it Inconclusive} is its complementary.
  The mean stop iteration is $\overline{n}$ and the standard deviation $\sigma_n$
  The mean sample rate when the stop is triggered is $\overline{\theta}$ and its standard deviation is $\sigma_{\hat{\theta}}$.
  }
\end{center}
\end{table}

% hdi_rope	0.0080	& 0.6480 &	0.6560 &	0.3440 &	804.6165	& 611.021115	& 0.604697	& 0.058295
% pitg	0.0005 &	0.1570 &	0.1575 &	0.8425	& 586.4470	& 7.116144 &	0.570288 &	0.020308
% epitg	0.0035 &	0.5415 &	0.5450 & 	0.4550 &	1127.1755 &	395.941462	& 0.575583 &	0.017217


\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{loaded0pt57_experiment_decision_rates.png}
  \caption{Similar information as in Figure \ref{fig:fair_iter_vs_rate} but focusing on
  the distribution of decisions. Left panel is for HDI+ROPE. Right panel are both
  precision based methods. Horizontal axis- iteration. Vertical axis- proportion of
  decisions made up to (and including) each iteration. The sum of Reject (red dashed),
  Accept (green solid) and Inconclusive/Collect-more- (gray dot-dashed) decisions at
  each iteration is 100\%.
  }
  \label{fig:loaded0pt57_decisions}
\end{figure}

\subsection{Results Summary}

We next examine trends of these algorithms when conducting the experiments between
$\theta_{\rm true}=0.5$ through $0.65$ at intervals of $0.1$. \footnote{Due to symmetry of the problem the results between $\theta_{\rm true}=0.35$ through $0.5$ would mirror the ones shown here.}
Assuming a null hypothesis of $\theta_{\rm true}=0.5$ we find that the PitG and ePitG
are effectively the same algorithm as they yields the same results. Below we demonstrate
where they differ.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{conclusive_rates.png}
  \caption{Conclusiveness Rates.
  }
  \label{fig:conclusiveness_rates}
\end{figure}

Figure \ref{fig:conclusiveness_rates} illustrates the conclusiveness rates of each
algorithm where the horizontal axis is the true success rate and the null hypothesis
is $\theta_{\rm null}=0.5$. 
As expected, all have a minimal extremum at the ROPE upper coundary of $\theta=0.55$ (vertical dashed line).
As seen in Figures TBD and TBD at $\theta_{\rm true}=0.5$ the HDI+ROPE and ePitG are nearly
100\% conclusive whereas PitG is less than 40\%. HDI+ROPE and ePitG assymptote
to 100\% conclusiveness again at $\theta_{\rm true}=0.6$, whereas for PitG this happens
at $\theta_{\rm true}=0.65$. Throughout HDI+ROPE is the most conclusive, however, it
is the most biased.




\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{success_by_truth_conclusive.png}
  \caption{Experiment success rates by true values.
  Left:  Interquartile ranges (IQR).The solid line is parity with $\theta_{\rm true}$. Dotted lines are the ROPE boundaries. Note that in the Inconclusive Experiments those of HDI+ROPE and ePitG are very narrow.
  Right: Deviations from the truth: Median minus $\theta_{\rm true}$.
  }
  \label{fig:success_by_truth_conclusive}
\end{figure}


tbd


\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{stop_conclusiveness_ratios.png}
  \caption{ePitG vs. PitG vs. iteration stop and conclusiveness rates.
  }
  \label{fig:stop_conclusiveness_ratios}
\end{figure}



Figure \ref{fig:success_by_truth} illustrates the biases of the algorithms.
In the left panels we demonstrate for all the experiments. We find it instrumentive
to split these results into two mutually exclusive subsets: conclusive experiments
(middle panels) and inconclusive (right panels).
In the top panels we visualise the interquartile ranges of each algorithm
and in the bottom panels the deviation of the medians from the true values. The ROPE
upper boundary is highlighted with a vertical dashed line to indicate an important
inflection point.



\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{success_by_truth.png}
  \caption{Success Rates Compared to True Rates.
  Left: Results for all experiments.
  Center: Results of subset conclusive experiments (for rates see Figure \ref{fig:conclusiveness_rates}).
  Right: Results of subset inconclusive experiments.
  Top:  Interquartile ranges (IQR).The solid line is parity with $\theta_{\rm true}$. Dotted lines are the ROPE boundaries. Note that in the Inconclusive Experiments those of HDI+ROPE and ePitG are very narrow.
  Bottom: Deviations from the truth: Median minus $\theta_{\rm true}$.
  }
  \label{fig:success_by_truth}
\end{figure}

The left panels show that the PitG yields unbiassed results. \cite{kruschke2015doing}
show this for $\theta_{\rm true}=0.5$ and $0.65$ and we show this for the full range.

If one differentiates between conclusive and inconclusive outcome the PitG yields
between $\hat{\theta}=0.55-0.63$ positively biassed results. The biases



The left panels demonstrate that when $\theta_{\rm true}=0.5$ all three algorithms,
when conclusive, yield unbiased outcomes. This was seen in Figure X and may be explained
due to the symmetric nature of this particular expriement. For other $\theta_{\rm true}$
values within the ROPE we see that, in the conclusive case, they, on average, bias towards
the null hypothesis $\theta_{\rm null}=0.5$ and hence correctly accept it.

The left panels demonstrate that even though HDI+ROPE is the most conclusive when
$\theta_{\rm true}$ is larger than the ROPE boundary it is much more biassed.
As explained in Section TBD this is due to outliers that happen to satisfy the location
condition without satisfying the precision. Even though this approach yields the
correct answer of rejecting the null hypothesis it raises concern of the quoted
result $\hat{\theta}$. When accounting for the precision objective we see that beyond
the ROPE boundary the PitG performs better than

