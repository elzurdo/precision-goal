Much of scientific progress is due to the ability to empirically test for hypotheses.
To this end statistical methods serve as bedrock tools to assess how representative a
sample is of an underlying truth. Since the accuracy of these assessments largely
depends on sample sizes one pressing question prior to conducting data collection.


In this study we focus on sequential testing, an alternative method to data
collection, by which  accumulation is done in a serial manner and a stop criterion is
determined and subsequently a decision criterion indicates the outcome.
This may be applied in clinical trials and medical diagnostics, quality control,
a/b testing (e.g, website decisions, digital marketing) as well as financial market
monitoring.

One potential benefit is efficiency, which may occur if the signal is stronger than
expected. Reducing the number of observations required to reach a conclusion may save
time and resources. Another advantage is flexibility and timeliness which
is useful in dynamic environments where conditions rapid changes may be
made as soon as sufficient evidence is available leading to quicker implementation of
findings. In the medical domain this may be
beneficial in terms of ethical considerations as it can prevent unnecessary exposure of
participants to ineffective or harmful treatments. 

With such benefits one must also consider the tradeoffs. Sequential analysis may be
complex and heavy operational demands as it requires more sophisticated statistical
methods and careful planning, if done by a human or an automated decision algorithm.
The focus of this study addresses an even more dangerious tradeoff --
the potential for bias decision making.

\cite{kruschke2015doing} points out that commonly applied Frequentist and Bayesian
methods to sequential sampling may yield biassed results. In the Frequentist case
practitioners set the stop criterion that triggers according to a sample to
hypothesis p-value and then decide if to reject the null hypothesis
(Frequentist methods cannot accept null hypotheses because p-values do not contain
enough information). In one  common Bayesian method, called HDI+ROPE which we elaborate
and demonstrate below, decisions on when to stop collecting data and the decision to
reject or accept the null hypothesis is when most of the posterior mass is fully inside
or outside of an “effective region” around the null hypothesis. In another Bayesian
method, called Bayesian Factor, one makes decisions based on ratios of posteriors and
priors metrics of a null hypothesis model compared to that of an alternative one.

\cite{kruschke2015doing} have shown that the common use of these heuristics as stop criteria may
trigger when a more extreme than average sample is obtained, which is more likely with
small sample sizes. From this we learn that premature stopping is likely to result in
biassed outcomes, which is commonly referred to as {\it confirmation bias} or
{\it early peaking} which reflects a practitioner’s eagerness for the data to agree with a
preferred hypothesis and stop collecting data when convenient. For science to make true
progress practitioners need to avoid such outcomes by using more appropriate methods.
