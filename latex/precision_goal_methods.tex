Here we describe three stopping algorithms for sequential hypothesis testing.
We organize the methods as follows:
\begin{enumerate}
    \item We outline the \textbf{Theoretical Framework}, defining the heuristics (ROPE, HDI) and the universal "Decision Rule" shared by all methods.
    \item We detail the three \textbf{Sequential Stopping Algorithms}: HDI+ROPE, Precision is the Goal, and Enhanced Precision is the Goal.
    \item We describe the \textbf{Experimental Design} used to evaluate these algorithms on synthetic data.
\end{enumerate}

\subsection{Theoretical Framework}

All three algorithms rely on Bayesian heuristics that describe two key aspects of the posterior distribution: its \textit{location} (central tendency relative to the null) and its \textit{width} (quantifying the epistemic uncertainty of the estimate).

\subsubsection{Region of Practical Equivalence (ROPE) and High Density Interval (HDI)}

A common misconception in hypothesis testing is that a ``statistically significant'' outcome is sufficient for real-world decision-making. In practice, one must consider the \textit{effect size}. For example, medical research requires a ``minimal clinically important difference'' for an outcome to justify a change in treatment.

Consider a hypothetical scenario where a researcher examines if a therapeutic differs in impact by gender. If we assume that enough data was collected to demonstrate that the difference is ``statistically significant'', e.g., the drug is 72.1\% beneficial for males and 72.3\% for females, a practitioner may consider this equivalent for all practical purposes. To justify treating the usage of the drug differently by gender, a meaningful absolute difference (e.g., 5\% or 10\%) would be required. Hence, the researcher should define in advance an ``effective area'' around the null hypothesis.

One such metric of interest to account for the importance of the effective size is the \textbf{Region of Practical Equivalence} (ROPE). The ROPE is defined as an area around the null value considered ``similar enough to the null hypothesis'' such that if the true value is within this area, it is effectively the same as the null hypothesis.

The width of the ROPE depends on the task at hand. For instance, a board game manufacturer requires dice to be fair to a reasonable standard, but not to a premium precision indistinguishable to a casual player. Conversely, a casino requires a much stricter tolerance (a narrower ROPE) to ensure regulatory compliance and fair play.

To value the uncertainty of our estimate, we use the \textit{credible interval} (the Bayesian analogue to the frequentist confidence interval). A popular heuristic is the \textbf{Highest Density Interval} (HDI), which describes the region where a substantial amount of the posterior resides; all points within it have a higher probability density than points outside.

The width of the HDI is commonly calculated using 95\% of the mass to be comparable with traditional Frequentist hypothesis testing. While reasonable for analytical solutions, \cite{kruschke2015doing} notes that for numerical solutions (MCMC), high precision requires at least 10,000 samples; otherwise, a lower mass percentage like 94\% is recommended to ensure stability. \cite{mcelreath2016} suggests 89\% (a prime number) to highlight the arbitrariness of the threshold. In this work, we adhere to 95\% as we employ analytical solutions where sampling noise is not a factor.

\subsubsection{The Decision Rule: Separating ``Stop'' from ``Decide''}\label{sec:decision_criterion}

Crucially, we distinguish between the \textbf{Stopping Rule} (when to stop collecting data) and the \textbf{Decision Rule} (what to conclude once stopped). While the three algorithms discussed later differ in their \textit{stopping} criteria, they all share an identical \textbf{Decision Rule}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{cherry_posteriors.png}
    \caption{Posteriors of subsamples of an example sequence. 
    Shaded areas are 95\% HDIs. The ROPE is within vertical dashed lines.
    Top: HDI fully outside of ROPE $\rightarrow$ Reject $\theta_{\rm null}$.
    Middle: HDI straddles ROPE $\rightarrow$ Inconclusive.
    Bottom: HDI fully within ROPE $\rightarrow$ Accept $\theta_{\rm null}$.
    }
    \label{fig:posteriors}
\end{figure}

Once the stopping criterion is triggered, the decision is determined solely by the relationship between the HDI and the ROPE (Figure \ref{fig:posteriors}):

\begin{itemize}
    \item \textbf{Reject Null}: The HDI is completely \textit{outside} the ROPE.
    \item \textbf{Accept Null}: The HDI is completely \textit{inside} the ROPE.
    \item \textbf{Inconclusive}: The HDI \textit{straddles} the ROPE boundary.
\end{itemize}

In Figure \ref{fig:posteriors} we display three posteriors from different stopping points of the same hand-picked experiment of Bernoulli trials ($\theta_{\rm null}=0.5$, ROPE width $= 0.1$).
\begin{itemize}
    \item In the \textbf{top panel}, we examine a stop at iteration 126. The 95\% HDI is fully outside the ROPE, leading to a rejection of the null hypothesis.
    \item In the \textbf{middle panel}, a stop is triggered at iteration 598. Here, the HDI straddles the ROPE boundary, resulting in an inconclusive decision.
    \item In the \textbf{bottom panel}, we stop at iteration 804. The 95\% HDI falls fully within the ROPE, allowing us to accept $\theta_{\rm null}$.
\end{itemize}

The insights from Figure \ref{fig:posteriors} illustrate that the decision outcome is time-dependent. An inconclusive outcome implies that the data collected is insufficient to justify a decision. In a live setting, a human-in-the-loop (or a wrapper algorithm, common in automated high-frequency environments) would need to either conduct a risk assessment or collect more data. Since risk assessment is highly contextual, we defer that discussion and focus on the necessity of collecting more data to avoid inconclusive outcomes.

Algorithm \ref{alg:decision_criterion} formalises this logic.

\begin{algorithm}
    \caption{The Decision Rule}\label{alg:decision_criterion}
    \begin{algorithmic}
    \Require $\mathrm{ROPE}_\mathrm{min}$, $\mathrm{ROPE}_\mathrm{max}$, $\mathrm{HDI}_\mathrm{min}$, $\mathrm{HDI}_\mathrm{max}$
    \If{$(\mathrm{ROPE}_\mathrm{min} < \mathrm{HDI}_\mathrm{min}) \ \& \ (\mathrm{HDI}_\mathrm{max} < \mathrm{ROPE}_\mathrm{max})$}
        \State Decision = Accept $\theta_{\rm null}$ \Comment{HDI completely within ROPE}
    \ElsIf{$\mathrm{ROPE}_\mathrm{max}<\mathrm{HDI}_\mathrm{min}$}
        \State Decision = Reject $\theta_{\rm null}$ \Comment{HDI completely outside ROPE; $\theta_{\rm null}<\hat\theta$}
    \ElsIf{$\mathrm{HDI}_\mathrm{max}< \mathrm{ROPE}_\mathrm{min}$}
        \State Decision = Reject $\theta_{\rm null}$ \Comment{HDI completely outside ROPE; $\hat\theta<\theta_{\rm null}$}
    \Else
    \State Decision = Inconclusive  \Comment{HDI straddles ROPE}
    \EndIf \\
    \Return Decision
    \end{algorithmic} 
\end{algorithm}

\subsection{Sequential Stopping Algorithms}

Table \ref{tab:comparison} provides a high-level comparison of the three algorithms. Each algorithm shares the same Decision Rule (Algorithm \ref{alg:decision_criterion}) but differs in when it triggers the stop condition.

\begin{table}[h!]
    \begin{center}
      \begin{tabular}{l|c|l|l}
        \textbf{Algorithm} & \textbf{Pre-Survey} & \textbf{Stop Criterion} &  \textbf{Decision Logic} \\
        \hline
        HDI + ROPE & $N_\mathrm{min}$ & Decision is conclusive & Stop $\to$ Decision \\
        Precision is the Goal & Goal & Precision $\le$ Goal & Stop $\to$ Decision \\
        Enhanced PitG & Goal & Precision $\le$ Goal \textbf{AND} & Stop $\to$ Decision \\
         & & Decision is conclusive & \\
      \end{tabular}
      \caption{Comparison of stopping algorithms. `Decision is conclusive' means the HDI is fully inside or fully outside the ROPE.}
      \label{tab:comparison}
    \end{center}
  \end{table}

In the Pre-Survey column we highlight the characteristic parameter required to be determined prior to the collection of data.
\begin{enumerate}
    \item \textbf{HDI + ROPE}: Requires $N_\mathrm{min}$ (minimal sample size), e.g., 30. This is crucial as the algorithm is sensitive to early outliers.
    \item \textbf{Precision Methods}: Require `Goal`, the target precision width, which must be narrower than the ROPE width $\Delta_\mathrm{ROPE}$.
\end{enumerate}

For brevity, the table excludes two universal parameters required for all three algorithms:
\begin{itemize}
    \item $N_\mathrm{max}$ (Final Budget): The maximum sample size or budget limit.
    \item $\Delta_\mathrm{ROPE}$ (Effect Size): The width of the Region of Practical Equivalence.
\end{itemize}

\subsubsection{HDI + ROPE: Location Only}

The \textbf{HDI + ROPE} method stops data collection as soon as the posterior's location allows for a decisive acceptance or rejection of the null hypothesis. It does not explicitly demand a specific degree of precision (width), only that the HDI does not straddle the ROPE boundary.

Algorithm \ref{alg:hdi_rope} details the procedure. Note the inclusion of $N_\mathrm{min}$ (minimal sample size); this is critical because, with small sample sizes, the HDI can fluctuate wildly and accidentally fall entirely within or outside the ROPE due to random noise (aleatoric uncertainty) rather than true effect.

\begin{algorithm}
    \caption{HDI + ROPE}\label{alg:hdi_rope}
    \begin{algorithmic}
    \Require $N_\mathrm{min}$, $N_\mathrm{max}$, $\theta_{\rm null}$, $\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{min} = \theta_{\rm null} - \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{max} = \theta_{\rm null} + \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State Stop = False
    \State Decision = Inconclusive
    \State N = 0
    \While{(Stop = False \& $N < N_\mathrm{max}$)} 
    \State N += 1 \Comment{collect another data point}
    \State Update $\hat\theta$, $P(\theta|\hat\theta)$ \Comment{update posterior}
    \State $\mathrm{HDI}_\mathrm{min}, \ \mathrm{HDI}_\mathrm{max}  \gets P(\theta|\hat\theta)$
    \If{$N \ge N_\mathrm{min}$}
        \State Decision = Decision Algorithm($\mathrm{ROPE}_\mathrm{min}, \mathrm{ROPE}_\mathrm{max}, \mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max}$)
        \If{Decision in \{Accept $\theta_{\rm null}$, Reject $\theta_{\rm null}$\}}
            \State \HiLi Stop = True \Comment{Stopping depends on decisive Decision}
        \EndIf
    \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}

This method is efficient but serves as a cautionary use case of ``confirmation bias'' or ``peeking'' intent if not bounded by $N_\mathrm{min}$.

\subsubsection{Precision Is The Goal (PitG): Precision Only}

Introduced by \cite{kruschke2015doing}, \textbf{Precision is the Goal} shifts the stopping condition from the \textit{decision} to the \textit{precision} (width) of the estimate. Data collection stops when the width of the HDI is narrower than a pre-specified Goal (e.g., 80\% of the ROPE width).

Algorithm \ref{alg:pitg} shows the ``De-coupled'' nature of this method: stopping depends on width, while the decision depends on location. This can lead to situations where the algorithm stops (precision reached) but the result is Inconclusive (straddling).

\begin{algorithm}
    \caption{Precision is the Goal}\label{alg:pitg}
    \begin{algorithmic}
    \Require Goal, $N_\mathrm{max}$, $\theta_{\rm null}$, $\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{min} = \theta_{\rm null} - \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{max} = \theta_{\rm null} + \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State Stop = False
    \State Decision = Inconclusive
    \State N = 0
    \While{(Stop = False \& $N < N_\mathrm{max}$)}
    \State N += 1 \Comment{collect another data point}
    \State Update $\hat\theta, P(\theta|\hat\theta)$ \Comment{update posterior}
    \State $\mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max} \gets P(\theta|\hat\theta)$
    \State Precision = $\mathrm{HDI}_\mathrm{max} - \mathrm{HDI}_\mathrm{min}$
    \If{Precision $\le$ Goal}
         \State \HiLi Stop = True \Comment{Stopping regardless of Decision}
         \State Decision = Decision Algorithm($\mathrm{ROPE}_\mathrm{min}, \mathrm{ROPE}_\mathrm{max}, \mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max}$)  
    \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}

\subsubsection{Enhanced Precision Is The Goal (ePitG): Precision and Location}

We propose \textbf{Enhanced Precision is the Goal}, a conservative variant of PitG, to mitigate the high rate of inconclusive results. This method ``re-couples'' the criteria: it stops strictly when \textit{both} the precision goal is met \textit{AND} a conclusive decision can be made.

As seen in Algorithm \ref{alg:epitg}, strictly enforcing both conditions ensures that we do not stop in an inconclusive state, potentially at the cost of collecting more data (up to $N_\mathrm{max}$).

\begin{algorithm}
    \caption{Enhanced Precision is the Goal}\label{alg:epitg}
    \begin{algorithmic}
    \Require Goal, $N_\mathrm{max}$, $\theta_{\rm null}$, $\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{min} = \theta_{\rm null} - \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{max} = \theta_{\rm null} + \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State Stop = False
    \State Decision = Inconclusive
    \State N = 0
    \While{(Stop = False \& $N < N_\mathrm{max}$)}
    \State N += 1 \Comment{collect another data point}
    \State Update $\hat\theta, P(\theta|\hat\theta)$ \Comment{update posterior}
    \State $\mathrm{HDI}_\mathrm{min}, \ \mathrm{HDI}_\mathrm{max}  \gets P(\theta|\hat\theta)$ 
    \State Precision = $\mathrm{HDI}_\mathrm{max} - \mathrm{HDI}_\mathrm{min}$
    \If{Precision $\le$ Goal}
        \State Decision = Decision Algorithm($\mathrm{ROPE}_\mathrm{min}, \mathrm{ROPE}_\mathrm{max}, \mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max}$) 
        \If{Decision in \{Accept $\theta_{\rm null}$, Reject $\theta_{\rm null}$\}} 
            \State \HiLi Stop = True \Comment{Stopping depends on Precision and Decision}
        \EndIf
    \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}

Note that all stopping algorithms (\ref{alg:hdi_rope}--\ref{alg:epitg}) assume the collection of one data point at a time, though this is easily modified in the increment step ($N \mathrel{+}= 1$) to accommodate batch collection.

\subsection{Experimental Design}

To evaluate and compare the performance of the three algorithms, we employ a synthetic data simulation based on Bernoulli trials (dichotomous data). While these methods are applicable to continuous data, we focus on the Bernoulli case to maintain comparability with the work of \cite{kruschke2015doing}.

We generate $M=500$ independent sequences of $N_{\rm max}=1,500$ trials each. This maximum sample size is chosen to simulate a typical medium-sized survey or experiment.

We test three scenarios for the ground truth probability $\theta_{\rm true}$ against a null hypothesis of $\theta_{\rm null}=0.5$:
\begin{enumerate}
    \item \textbf{Fair Coin ($\theta_{\rm true}=0.5$)}: The null hypothesis is true.
    \item \textbf{High Loading ($\theta_{\rm true}=0.6$)}: A strong effect exists.
    \item \textbf{Slight Loading ($\theta_{\rm true}=0.52$)}: A weak effect exists, challenging the sensitivity of the algorithms.
\end{enumerate}

\subsubsection{Algorithm Configuration}

Unless stated otherwise, we utilize the following default parameters, chosen to reflect detailed but realistic experimental conditions (e.g., opinion polling):

\begin{itemize}
    \item \textbf{ROPE Width ($\Delta_{\rm ROPE}=0.1$)}: We define the Region of Practical Equivalence as $\theta_{\rm null} \pm 0.05$. This implies that any result within the range $[0.45, 0.55]$ is considered practically equivalent to the null.
    \item \textbf{Precision Goal}: We set the target precision (HDI width) to 80\% of the ROPE width ($\mathrm{Goal}=0.08$).
    \item \textbf{Minimum Sample Size ($N_{\rm min}=30$)}: A practical lower bound to prevent premature stopping due to early stochastic noise, particularly relevant for the location-based HDI+ROPE algorithm.
\end{itemize}

In the following section, we present the results of these simulations, first under these default settings and subsequently by varying key parameters to explore robustness.


