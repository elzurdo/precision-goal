Here we describe three stopping algorithms for sequential hypothesis testing.
We organize the methods as follows:
\begin{enumerate}
    \item We outline the \textbf{Theoretical Framework}, defining the heuristics (ROPE, HDI) and the universal "Decision Rule" shared by all methods.
    \item We detail the three \textbf{Sequential Stopping Algorithms}: HDI+ROPE, Precision is the Goal, and Enhanced Precision is the Goal.
    \item We describe the \textbf{Experimental Design} used to evaluate these algorithms on synthetic data.
\end{enumerate}

\subsection{Theoretical Framework}

All three algorithms rely on Bayesian heuristics that describe two key aspects of the posterior distribution: its \textit{location} (central tendency relative to the null) and its \textit{width} (quantifying the epistemic uncertainty of the estimate).

\subsubsection{Region of Practical Equivalence (ROPE) and High Density Interval (HDI)}

A common misconception in hypothesis testing is that a ``statistically significant'' outcome is sufficient for real-world decision-making. In practice, one must consider the \textit{effect size}. For example, medical research requires a ``minimal clinically important difference'' for an outcome to justify a change in treatment.

Consider a hypothetical scenario where a researcher examines if a therapeutic differs in impact by gender. If we assume that enough data was collected to demonstrate that the difference is ``statistically significant'', e.g., the drug is 72.1\% beneficial for males and 72.3\% for females, a practitioner may consider this equivalent for all practical purposes. To justify treating the usage of the drug differently by gender, a meaningful absolute difference (e.g., 5\% or 10\%) would be required. Hence, the researcher should define in advance an ``effective area'' around the null hypothesis.

One such metric of interest to account for the importance of the effective size is the \textbf{Region of Practical Equivalence} (ROPE). The ROPE is defined as an area around the null value considered ``similar enough to the null hypothesis'' such that if the true value is within this area, it is effectively the same as the null hypothesis.

The width of the ROPE depends on the task at hand. For instance, a board game manufacturer requires dice to be fair to a reasonable standard, but not to a premium precision indistinguishable to a casual player. Conversely, a casino requires a much stricter tolerance (a narrower ROPE) to ensure regulatory compliance and fair play.

To value the uncertainty of our estimate, we use the \textit{credible interval} (the Bayesian analogue to the frequentist confidence interval). A popular heuristic is the \textbf{Highest Density Interval} (HDI), which describes the region where a substantial amount of the posterior resides; all points within it have a higher probability density than points outside.

The width of the HDI is commonly calculated using 95\% of the mass to be comparable with traditional Frequentist hypothesis testing. While reasonable for analytical solutions, \cite{kruschke2015doing} notes that for numerical solutions (MCMC), high precision requires at least 10,000 samples; otherwise, a lower mass percentage like 94\% is recommended to ensure stability. \cite{mcelreath2016} suggests 89\% (a prime number) to highlight the arbitrariness of the threshold. In this work, we adhere to 95\% as we employ analytical solutions where sampling noise is not a factor.

\subsubsection{The Decision Rule: Separating ``Stop'' from ``Decide''}\label{sec:decision_criterion}

Crucially, we distinguish between the \textbf{Stopping Rule} (when to stop collecting data) and the \textbf{Decision Rule} (what to conclude once stopped). While the three algorithms discussed later differ in their \textit{stopping} criteria, they all share an identical \textbf{Decision Rule}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{cherry_posteriors.png}
    \caption{Posteriors of subsamples of an example sequence. 
    Shaded areas are 95\% HDIs. The ROPE is within vertical dashed lines.
    Top: HDI fully outside of ROPE $\rightarrow$ Reject $\theta_{\rm null}$.
    Middle: HDI straddles ROPE $\rightarrow$ Inconclusive.
    Bottom: HDI fully within ROPE $\rightarrow$ Accept $\theta_{\rm null}$.
    }
    \label{fig:posteriors}
\end{figure}

Once the stopping criterion is triggered, the decision is determined solely by the relationship between the HDI and the ROPE (Figure \ref{fig:posteriors}):

\begin{itemize}
    \item \textbf{Reject Null}: The HDI is completely \textit{outside} the ROPE.
    \item \textbf{Accept Null}: The HDI is completely \textit{inside} the ROPE.
    \item \textbf{Inconclusive}: The HDI \textit{straddles} the ROPE boundary.
\end{itemize}

In Figure \ref{fig:posteriors} we display three posteriors from different stopping points of the same hand-picked experiment of Bernoulli trials ($\theta_{\rm null}=0.5$, ROPE width $= 0.1$).
\begin{itemize}
    \item In the \textbf{top panel}, we examine a stop at iteration 126. The 95\% HDI is fully outside the ROPE, leading to a rejection of the null hypothesis.
    \item In the \textbf{middle panel}, a stop is triggered at iteration 598. Here, the HDI straddles the ROPE boundary, resulting in an inconclusive decision.
    \item In the \textbf{bottom panel}, we stop at iteration 804. The 95\% HDI falls fully within the ROPE, allowing us to accept $\theta_{\rm null}$.
\end{itemize}

The insights from Figure \ref{fig:posteriors} illustrate that the decision outcome is time-dependent. An inconclusive outcome implies that the data collected is insufficient to justify a decision. In a live setting, a human-in-the-loop (or a wrapper algorithm, common in automated high-frequency environments) would need to either conduct a risk assessment or collect more data. Since risk assessment is highly contextual, we defer that discussion and focus on the necessity of collecting more data to avoid inconclusive outcomes.

Algorithm \ref{alg:decision_criterion} formalises this logic.

\begin{algorithm}
    \caption{The Decision Rule}\label{alg:decision_criterion}
    \begin{algorithmic}
    \Require $\mathrm{ROPE}_\mathrm{min}$, $\mathrm{ROPE}_\mathrm{max}$, $\mathrm{HDI}_\mathrm{min}$, $\mathrm{HDI}_\mathrm{max}$
    \If{$(\mathrm{ROPE}_\mathrm{min} < \mathrm{HDI}_\mathrm{min}) \ \& \ (\mathrm{HDI}_\mathrm{max} < \mathrm{ROPE}_\mathrm{max})$}
        \State Decision = Accept $\theta_{\rm null}$ \Comment{HDI completely within ROPE}
    \ElsIf{$\mathrm{ROPE}_\mathrm{max}<\mathrm{HDI}_\mathrm{min}$}
        \State Decision = Reject $\theta_{\rm null}$ \Comment{HDI completely outside ROPE; $\theta_{\rm null}<\hat\theta$}
    \ElsIf{$\mathrm{HDI}_\mathrm{max}< \mathrm{ROPE}_\mathrm{min}$}
        \State Decision = Reject $\theta_{\rm null}$ \Comment{HDI completely outside ROPE; $\hat\theta<\theta_{\rm null}$}
    \Else
    \State Decision = Inconclusive  \Comment{HDI straddles ROPE}
    \EndIf \\
    \Return Decision
    \end{algorithmic} 
\end{algorithm}

\subsection{The Algorithms}
\subsubsection{A High Level Comparison}

As mentioned above, when the stop criteria trigger, all three test algorithms
apply the same decision criterion but in slightly different manners.


Table \ref{tab:table1} provides a high level comparison between the test algorithms.


\begin{table}[h!]\label{tab:table1}
    \begin{center}
      \begin{tabular}{c|c|c|c}
        \textbf{Test Algorithm } & \textbf{Pre Survey} & \textbf{Stop Criterion} &  \textbf{Decision Criterion}\\
        \hline
        HDI + ROPE & $N_\mathrm{min}$  & \multicolumn{2}{c}{\multirow{1}{*}{HDI within/outside ROPE?}}  \\
        Precision is the Goal & Goal & Precision $<$ Goal? & HDI within/outside ROPE? \\
        Enhanced Precision is the Goal & Goal& \multicolumn{2}{c}{\multirow{1}{*}{(Precision $<$ Goal?) \& (HDI within/outside ROPE?)}}  \\
      \end{tabular}
      \caption{A high level comparison between the test algorithms.
      The merged cells mean Stop and Decision criteria are simultaneous.
      Otherwise multi-step.
      }
    \end{center}
  \end{table}

In the Pre Survey column we highlight the a characteristic parameter required to be
determined prior to the collection of data. For HDI+ROPE this is the minimal sample size
to collect $N_\mathrm{min}$, e.g, 30. The reason for this is that this algorithm
is very sensitive to strong outliers.

For the two precision based algorithms the $\mathrm{GOAL}$ precision is required,
which should be less than the $\mathrm{ROPE}$ width $\Delta_\mathrm{ROPE}$. For brevity of the table
readability we excluded two additional Pre Survey parameters that are required for all three
algorithms:
\begin{itemize}
    \item $N_\mathrm{max}$ - the maximum sample size, i.e, the final budget size. In most practical cases one will want an upper bound of how much data to collect
    \item $\Delta_\mathrm{ROPE}$ - The ROPE width which is a statement of minimum effect size considered meaningful
\end{itemize}

In the Stop and Decision Criteria columns we see that,
as per \S\ref{sec:decision_criterion}, the decision criterion is the same
for all three. That is to say that all three make the decision based on the
location of the HDI with respect to the ROPE, i.e, the posterior location in respect
to the null hypothesis and its effective region.
Whereas this is the only criterion for the HDI+ROPE algorithm, the other two
are more conservative as they require also the posterior width information in the form
{\it precision}, which we will later define.


Note that the Precision is the Goal method is a two step process, 
i.e, "Stop" then "Decide",
and the other two activate both Stop and Decision criteria simultaneously
(i.e, "Stop and Decide"). In the next three sections we explain each in detail,
followed up with analytic and synthetic examples.


\subsubsection{HDI + ROPE: Location, Location, Location}

HDI + ROPE is a popular method that combines the HDI and ROPE heuristics to describe
the relationship between the \textbf{location} of posterior $P(\theta|\hat\theta)$, which depends on the observed
sample value $\hat\theta$ in regards to the
null hypothesis $\theta_{\rm null}$.

In Algorithm \ref{alg:hdi_rom} we provide a pseudo code of the HDI + ROPE algorithm.



\begin{algorithm}
    \caption{HDI + ROM pseudo algorithm}\label{alg:hdi_rom}
    \begin{algorithmic}
    \Require $N_\mathrm{min}$, $N_\mathrm{max}$, $\theta_{\rm null}$, $\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{min} = \theta_{\rm null} - \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{max} = \theta_{\rm null} + \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State Stop = False
    \State Decision = Inconclusive
    \State N = 0
    \While{(Stop = False \& $N \le N_\mathrm{max}$)}  % perhaps change to while loop
    \State N += 1 \Comment{collect another data point} 
    \State Update $\hat\theta$, $P(\theta|\hat\theta)$ \Comment{update posterior} 
    \State $\mathrm{HDI}_\mathrm{min}, \ \mathrm{HDI}_\mathrm{max}  \gets P(\theta|\hat\theta)$
    \If{$N_\mathrm{min} < N$}
        \State Decision = Decision Algorithm($\mathrm{ROPE}_\mathrm{min}, \mathrm{ROPE}_\mathrm{max}, \mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max}$)
        \If{Decision in \{Accept $\theta_{\rm null}$, Reject $\theta_{\rm null}$\}}
            \State \HiLi Stop = True  \Comment{Stopping depends on decisive Decision}
        \EndIf
    \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}


It requires a few decisions to be made prior to the collection of data (Pre Survey Decisions):

\begin{itemize}
    \item $N_\mathrm{min}$ - the minimal sample size to collect
    \item $N_\mathrm{max}$ - the maximum sample size
    \item $\theta_{\rm null}$ - the null hypothesis parameter value being tested
    \item $\Delta_\mathrm{ROPE}$ - The ROPE width (minimum effect size)
\end{itemize}

Algorithm \ref{alg:hdi_rom} assumes collection of one data point at a time,
but this may be modified in the line $N += 1$ to be for a batch collection.

The most important aspects of this algorithm are that:

\begin{itemize}
    \item The Stop and Decision criterion are one and the same
    \item The Stop criterion depends only on location information, not on the precision of the posterior.
\end{itemize}

Due to its disregard of dependence on the uncertainty of the posterior,
this popular method serves as a cautionary use case of the ``confirmation bias"
which is addressed by the Precision based methods described next.


\subsubsection{Precision Is The Goal: Width Then Location}

\cite{kruschke2015doing} introduced the Precision is the Goal algorithm to account
of the importance of the width of the posterior as the objective of the Stop Criterion.
Building upon the HDI + ROPE algorithm it uses the same Decision Criterion.

We describe a pseudo algorithm of the Precision is the Goal algorithm in Algorithm \ref{alg:pitg}.

\begin{algorithm}
    \caption{Preicion is the Goal pseudo algorithm}\label{alg:pitg}
    \begin{algorithmic}
    \Require Goal, $N_\mathrm{max}$, $\theta_{\rm null}$, $\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{min} = \theta_{\rm null} - \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{max} = \theta_{\rm null} + \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State Stop = False
    \State Decision = Inconclusive
    \State N = 0
    \While{(Stop = False \& $N \le N_\mathrm{max}$)}  % perhaps change to while loop
    \State N += 1 \Comment{collect another data point} 
    \State Update $\hat\theta$, $P(\theta|\hat\theta)$ \Comment{update posterior} 
    \State $\mathrm{HDI}_\mathrm{min}, \ \mathrm{HDI}_\mathrm{max}  \gets P(\theta|\hat\theta)$
    \State Precision = $\mathrm{HDI}_\mathrm{max} - \mathrm{HDI}_\mathrm{min}$
    \If{Precision $ \le $ Goal}
         \State \HiLi Stop = True \Comment{Stopping regardless of Decision}
         \State Decision = Decision Algorithm($\mathrm{ROPE}_\mathrm{min}, \mathrm{ROPE}_\mathrm{max}, \mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max}$)  
    \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}

Here we see that Precision is the Goal stops collecting when the posterior width is less than
the set goal regardless of the decision criterion, making it a two step process.


In our analyses in section [TBD] we show that, depending on the setting,
the Precision is the Goal algorithm may yield a high percentage of inconclusive results.

In the next subsection we suggest a slighlty more conservative variant of the algorithm
that acts as the original but collects more data to increase the rates
of decisive decisions.

\subsubsection{Enhanced Precision Is The Goal: Width \& Location}

We modify the \cite{kruschke2015doing} Precision is the Goal algorithm
to make the stop criterion dependent both on the with of the posterior
as well as its location in repsect to the null hypothesis.

\begin{algorithm}
    \caption{Enhanced Preicion is the Goal pseudo algorithm}\label{alg:epitg}
    \begin{algorithmic}
    \Require Goal, $N_\mathrm{max}$, $\theta_{\rm null}$, $\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{min} = \theta_{\rm null} - \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State $\mathrm{ROPE}_\mathrm{max} = \theta_{\rm null} + \frac{1}{2}\Delta_\mathrm{ROPE}$
    \State Stop = False
    \State Decision = Inconclusive
    \State N = 0
    \While{(Stop = False \& $N \le N_\mathrm{max}$)}  % perhaps change to while loop
    \State N += 1 \Comment{collect another data point} 
    \State Update $\hat\theta$, $P(\theta|\hat\theta)$ \Comment{update posterior} 
    \State $\mathrm{HDI}_\mathrm{min}, \ \mathrm{HDI}_\mathrm{max}  \gets P(\theta|\hat\theta)$ 
    \State Precision = $\mathrm{HDI}_\mathrm{max} - \mathrm{HDI}_\mathrm{min}$
    \If{Precision $ \le $ Goal}
        \State Decision = Decision Algorithm($\mathrm{ROPE}_\mathrm{min}, \mathrm{ROPE}_\mathrm{max}, \mathrm{HDI}_\mathrm{min}, \mathrm{HDI}_\mathrm{max}$) 
        \If{Decision in \{Accept $\theta_{\rm null}$, Reject $\theta_{\rm null}$\}} 
            \State \HiLi Stop = True \Comment{Stopping depends on Precision and Decision}
        \EndIf
    \EndIf
    \EndWhile
    \end{algorithmic}
\end{algorithm}

We see that the stopping and decision criteria act simultaneously, similar to the HDI + ROPE Algorithm \ref{alg:hdi_rom},
but requires the precision condition as per the Precision is the Goal Algorithm \ref{alg:pitg}.

A highlevel summary of the three algorithms as described in Table \ref{tab:table1}
and Algorithms \ref{alg:hdi_rom}, \ref{alg:pitg}, and \ref{alg:epitg} is that

\begin{itemize}
    \item HDI + ROPE is a one step process that stops based on the location of the posterior
    \item Precision is the Goal is a two step process that stops based on the width of the posterior
    \item Enhanced Precision is the Goal is a one step process that stops based on the width and location of the posterior
\end{itemize}

We next turn to simulating sequential hypothesis testing using synthetic data as well
analytical descriptions of the expected outcomes.
\subsection{Synthetic Data and Experiment Setup}

To compare and invetigate the implications of three algorithms we
follow the setup of \cite{kruschke2015doing} and generate synthetic data
based on a Bernoulli trials with known true values of $\theta_{\rm true}$.

We generate $M$ sequences of $N_{\rm max}$ Bernoulli trials with a true value of $\theta_{\rm true}$.

Values used are

\begin{itemize}
    \item $N_{\rm max}=1,500$ to simulate a medium sized survey
    \item $M=500$ sequences to ensure statistical robusteness of results
    \item $\theta_{\rm true}=0.5, 0.52, 0.6$ to simulate a fair, slightly and highly loaded coins
\end{itemize}

The default value of the ROPE width is $\Delta_{\rm ROPE}=0.1$. This indicates
the effect size that is considered meaningful. (TK minimum maximum?).

The default value of the Precision Goal is 80\% of the ROPE width, i.e, $\mathrm{Goal}=0.08$.

The default value of the minimal sample size is $N_{\rm min}=30$. This is a practical
setting to avoid many weak outliers to impact HDI+ROPE (the precision based algorithms
are shielded from requiring this as inhereitely require larger sample sizes).

These values were chosen as considered reasonable settings for some real life
situations. (TK examples? Polling data)

We start with the default setting and then vary the parameters to investigate to
explore implications for other settings (TK examples?).


